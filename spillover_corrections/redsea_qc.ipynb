{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e665d12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import scanpy as sc\n",
    "import rapids_singlecell as rsc\n",
    "import anndata\n",
    "import os\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c7b9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def z_normalize(data, list_out, list_keep):\n",
    "    \"\"\"\n",
    "    Normalize data by z-scoring and filter columns based on provided lists.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Input DataFrame containing data to be normalized.\n",
    "        list_out (list): List of columns to be dropped from the DataFrame.\n",
    "        list_keep (list): List of columns to be kept in the DataFrame but not normalized.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with columns normalized by z-score and filtered based on the provided lists.\n",
    "    \"\"\"\n",
    "    # Drop columns containing 'blank' in their names and additional columns specified in list_out\n",
    "    list1 = [col for col in data.columns if 'blank' in col]\n",
    "    list_out1 = list1 + list_out\n",
    "    dfin = data.drop(list_out1, axis=1)\n",
    "\n",
    "    # Save columns specified in list_keep for later\n",
    "    df_loc = dfin.loc[:, list_keep]\n",
    "\n",
    "    # DataFrame for normalization (drop columns specified in list_keep)\n",
    "    dfz = dfin.drop(list_keep, axis=1)\n",
    "\n",
    "    # Calculate z-scores for the column markers\n",
    "    dfz1 = pd.DataFrame(zscore(dfz, 0), index=dfz.index, columns=[i for i in dfz.columns])\n",
    "\n",
    "    # Add back labels for normalization type\n",
    "    dfz_all = pd.concat([dfz1, df_loc], axis=1, join='inner')\n",
    "\n",
    "    # Print the number of unique File_ID\n",
    "    #print(\"the number of unique regions = \" + str(len(dfz_all.unique_region.unique())))\n",
    "\n",
    "    return dfz_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc3ae2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def remove_noise(df, marker_cols, z_sum_thres, z_count_thres):\n",
    "    \"\"\"\n",
    "    Remove noisy cells from the dataset based on specified thresholds.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the dataset.\n",
    "        marker_cols (list): List of columns in the DataFrame to consider for noise removal.\n",
    "        z_sum_thres (int): Threshold for the sum of z-scores.\n",
    "        z_count_thres (int): Threshold for the count of values greater than 1.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with noisy cells removed.\n",
    "        DataFrame: DataFrame containing removed noisy cells.\n",
    "    \"\"\"\n",
    "    # Calculate count of values greater than 1 and sum of z-scores for each row\n",
    "    df['Count'] = df[marker_cols].ge(1).sum(axis=1)\n",
    "    df['z_sum'] = df[marker_cols].sum(axis=1)\n",
    "\n",
    "    # Filter out noisy cells based on z_sum and Count thresholds\n",
    "    cc = df[(df['z_sum'] > z_sum_thres) & (df['Count'] > z_count_thres)]\n",
    "    df_want = df[~((df['z_sum'] > z_sum_thres) & (df['Count'] > z_count_thres))]\n",
    "\n",
    "    # Calculate the percentage of removed noisy cells\n",
    "    percent_removed = len(df_want) / len(df)\n",
    "    print(str(percent_removed))\n",
    "\n",
    "    # Plot histogram of cell counts\n",
    "    ee = df['Count'].plot.hist(bins=50, alpha=0.8, logy=False)\n",
    "\n",
    "    # Drop Count and z_sum columns and reset index for the filtered DataFrame\n",
    "    df_want.drop(columns=['Count', 'z_sum'], inplace=True)\n",
    "    df_want.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df_want, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7f4e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def nuclear_size(data_in, size_thres, nuc_thres):\n",
    "    \"\"\"\n",
    "    Plot scatter plot by region, filter data based on size and nuclear intensity thresholds.\n",
    "\n",
    "    Parameters:\n",
    "        data_in (DataFrame): Input DataFrame containing nuclear size and intensity data.\n",
    "        size_thres (float): Threshold value for nuclear size.\n",
    "        nuc_thres (float): Threshold value for nuclear intensity.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing nuclei that meet the size and intensity criteria.\n",
    "    \"\"\"\n",
    "    # Plot scatter plot by region\n",
    "    plt.figure(figsize=(7,7))\n",
    "    data_plot = data_in.sample(frac=0.05, random_state=42)\n",
    "    g = sns.scatterplot(data=data_plot, x='area', y='DAPI', size=1)\n",
    "    g.set_xscale('log')\n",
    "    g.set_yscale('log')\n",
    "    ticks = [0.1, 1, 10, 100, 1000, 10000]\n",
    "    g.set_yticks(ticks)\n",
    "    g.set_yticklabels(ticks)\n",
    "    g.set_xticks(ticks)\n",
    "    g.set_xticklabels(ticks)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "    # Add vertical line at size threshold\n",
    "    plt.axvline(x=size_thres, color='red')\n",
    "\n",
    "    # Add horizontal line at nuclear intensity threshold\n",
    "    plt.axhline(y=nuc_thres, color='red')\n",
    "\n",
    "    # Filter data based on size and nuclear intensity thresholds\n",
    "    df_nuc = data_in[(data_in['area'] > size_thres) & (data_in['DAPI'] > nuc_thres)]\n",
    "    per_keep = len(df_nuc) / len(data_in)\n",
    "    print(per_keep)\n",
    "\n",
    "    return df_nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3a0a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_adata(data, list_keep_obs):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to AnnData format.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Input DataFrame containing data to be converted.\n",
    "        list_keep_obs (list): List of columns to be kept as observation attributes.\n",
    "\n",
    "    Returns:\n",
    "        AnnData: AnnData object containing data and observation attributes.\n",
    "    \"\"\"\n",
    "    # Reset the index of the DataFrame\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Prepare for making into anndata format by dropping columns specified in list_keep_obs\n",
    "    dfsc = data.drop(columns=list_keep_obs)\n",
    "\n",
    "    # Convert to AnnData format\n",
    "    adata = sc.AnnData(dfsc)\n",
    "\n",
    "    # Assign observation attributes to the AnnData object\n",
    "    adata.obs = data[list_keep_obs]\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0065f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def adata_to_dataframe(adata):\n",
    "    \"\"\"\n",
    "    Convert AnnData object to DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        adata (AnnData): AnnData object to be converted.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing both variables and observation attributes.\n",
    "    \"\"\"\n",
    "    # Convert AnnData variables to DataFrame\n",
    "    df_var = adata.to_df()\n",
    "\n",
    "    # Extract observation attributes from AnnData\n",
    "    df_obs = adata.obs\n",
    "\n",
    "    # Concatenate variable DataFrame and observation attribute DataFrame along axis 1\n",
    "    df = pd.concat([df_var, df_obs], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea6879",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === USER-DEFINED PATHS ===\n",
    "# This should be the directory that contains the per-slide subfolders (e.g., B004-A-004, B004-A-008, ...)\n",
    "input_root = \"/mnt/jwh83-data/Orthopaedic_QuPath/QuPath_Project/REDSEA_CSVS\"\n",
    "output_dir  = \"/mnt/jwh83-data/Orthopaedic_QuPath/QuPath_Project/CSV\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "before_out = os.path.join(output_dir, \"single_cell_before_redsea_ALL.csv\")\n",
    "after_out  = os.path.join(output_dir, \"single_cell_after_redsea_ALL.csv\")\n",
    "\n",
    "# Filenames inside each subfolder\n",
    "BEFORE_NAME = \"single_cell_before_redsea.csv\"\n",
    "AFTER_NAME  = \"single_cell_after_redsea.csv\"\n",
    "\n",
    "before_list, after_list = [], []\n",
    "\n",
    "def load_and_tag(csv_path: str, slide_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Add slide_name column (parent folder name)\n",
    "    df[\"slide_name\"] = slide_name\n",
    "\n",
    "    # Update CellID -> \"{slide_name}_{original}\"\n",
    "    if \"CellID\" in df.columns:\n",
    "        df[\"CellID\"] = slide_name + \"_\" + df[\"CellID\"].astype(str)\n",
    "    else:\n",
    "        print(f\"⚠️ Missing 'CellID' column in: {csv_path}\")\n",
    "\n",
    "    # Optional: keep provenance\n",
    "    df[\"FILE\"] = os.path.basename(csv_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "# === ITERATE SUBFOLDERS ===\n",
    "for entry in sorted(os.listdir(input_root)):\n",
    "    slide_dir = os.path.join(input_root, entry)\n",
    "    if not os.path.isdir(slide_dir):\n",
    "        continue\n",
    "\n",
    "    slide_name = entry  # parent subfolder name\n",
    "\n",
    "    before_path = os.path.join(slide_dir, BEFORE_NAME)\n",
    "    after_path  = os.path.join(slide_dir, AFTER_NAME)\n",
    "\n",
    "    if os.path.exists(before_path):\n",
    "        try:\n",
    "            print(f\"Processing BEFORE: {before_path}\")\n",
    "            before_list.append(load_and_tag(before_path, slide_name))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {before_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Missing BEFORE file for {slide_name}: {before_path}\")\n",
    "\n",
    "    if os.path.exists(after_path):\n",
    "        try:\n",
    "            print(f\"Processing AFTER:  {after_path}\")\n",
    "            after_list.append(load_and_tag(after_path, slide_name))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {after_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Missing AFTER file for {slide_name}: {after_path}\")\n",
    "\n",
    "# === COMBINE & SAVE ===\n",
    "if before_list:\n",
    "    before_df = pd.concat(before_list, ignore_index=True)\n",
    "    before_df.to_csv(before_out, index=False)\n",
    "    print(f\"✅ Combined BEFORE CSV saved to: {before_out}\")\n",
    "else:\n",
    "    print(\"❌ No BEFORE CSV files were processed.\")\n",
    "\n",
    "if after_list:\n",
    "    after_df = pd.concat(after_list, ignore_index=True)\n",
    "    after_df.to_csv(after_out, index=False)\n",
    "    print(f\"✅ Combined AFTER CSV saved to: {after_out}\")\n",
    "else:\n",
    "    print(\"❌ No AFTER CSV files were processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
